"""
DBì— ì €ì¥ëœ ì¥ì†Œë“¤ì— ëŒ€í•´ ë¦¬ë·° í¬ë¡¤ë§ ë° ì €ì¥
------------------------------------------
1. DBì—ì„œ ëª¨ë“  ì¥ì†Œ ID ê°€ì ¸ì˜¤ê¸°
2. ê° ì¥ì†Œì— ëŒ€í•´ ë¦¬ë·° í¬ë¡¤ë§
3. í¬ë¡¤ë§í•œ ë¦¬ë·°ë¥¼ DBì— ì €ì¥
"""

from __future__ import annotations

import argparse
import asyncio
import sys
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv

# backend í´ë”ì˜ .env íŒŒì¼ ë¡œë“œ
BACKEND_ROOT = Path(__file__).resolve().parent.parent
load_dotenv(BACKEND_ROOT / ".env")

# backend ëª¨ë“ˆ importë¥¼ ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(BACKEND_ROOT))

from sqlalchemy.orm import Session

from app.db.session import SessionLocal
from app.models.place import Place
from app.models.review import Review

# review_crawl.pyì˜ í¬ë¡¤ëŸ¬ import (ê°™ì€ scripts í´ë” ì•ˆì— ìˆìŒ)
# scripts í´ë”ë¥¼ sys.pathì— ì¶”ê°€
SCRIPTS_DIR = Path(__file__).resolve().parent
if str(SCRIPTS_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPTS_DIR))
from review_crawl import NaverMapReviewCrawler
from datetime import datetime


def parse_visit_date(date_str: str | None) -> datetime | None:
    """ë°©ë¬¸ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜."""
    if not date_str:
        return None
    # "1.24.í† " í˜•ì‹ ì²˜ë¦¬
    try:
        parts = date_str.split(".")
        if len(parts) >= 2:
            month = int(parts[0])
            day = int(parts[1])
            # í˜„ì¬ ì—°ë„ ì‚¬ìš© (ì •í™•í•˜ì§€ ì•Šì§€ë§Œ í¬ë¡¤ë§ ì‹œì  ê¸°ì¤€)
            year = datetime.now().year
            return datetime(year, month, day)
    except (ValueError, IndexError):
        pass
    return None


def upsert_review_to_db(db: Session, place_id: int, review_data: dict) -> Review | None:
    """ë¦¬ë·° ë°ì´í„°ë¥¼ DBì— ì €ì¥ ë˜ëŠ” ì—…ë°ì´íŠ¸."""
    review_id_str = review_data.get("id") or review_data.get("review_id")
    if not review_id_str:
        return None

    try:
        # ê¸°ì¡´ ë¦¬ë·° í™•ì¸
        existing = db.query(Review).filter(Review.review_id == review_id_str).first()
        
        visit_date = None
        if review_data.get("visit_date"):
            # review_crawl.pyì˜ parse_visit_date ì‚¬ìš©
            visit_date = parse_visit_date(review_data.get("visit_date"))
        
        crawled_at = datetime.now()

        if existing:
            # ì—…ë°ì´íŠ¸
            existing.content = review_data.get("content", "")
            existing.author = review_data.get("author")
            existing.rating = review_data.get("rating")
            existing.visit_date = visit_date
            existing.crawled_at = crawled_at
            db.commit()
            db.refresh(existing)
            return existing
        else:
            # ìƒˆë¡œ ìƒì„±
            review = Review(
                place_id=place_id,
                review_id=review_id_str,
                author=review_data.get("author"),
                content=review_data.get("content", ""),
                rating=review_data.get("rating"),
                visit_date=visit_date,
                crawled_at=crawled_at,
            )
            db.add(review)
            db.commit()
            db.refresh(review)
            return review
    except Exception:
        db.rollback()
        raise


async def crawl_reviews_for_place(
    crawler: NaverMapReviewCrawler,
    db: Session,
    place_id: int,
    max_count: int = 100,
) -> tuple[int, int]:
    """íŠ¹ì • ì¥ì†Œì— ëŒ€í•´ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•˜ê³  DBì— ì €ì¥."""
    # DBì— ì´ë¯¸ ì €ì¥ëœ ë¦¬ë·° ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ë°©ì§€)
    existing_review_ids = {
        r.review_id for r in db.query(Review.review_id).filter(Review.place_id == place_id).all()
    }
    
    try:
        reviews = await crawler.crawl_all_reviews(str(place_id), existing_review_ids, max_count=max_count)
    except Exception as exc:
        print(f"[ERROR] place_id={place_id} í¬ë¡¤ë§ ì‹¤íŒ¨: {exc}", file=sys.stderr)
        import traceback
        if len(str(exc)) < 200:  # ì§§ì€ ì—ëŸ¬ë§Œ ìƒì„¸ ì¶œë ¥
            print(traceback.format_exc(), file=sys.stderr)
        return 0, 0

    if not reviews:
        return 0, 0

    success = 0
    failed = 0

    for review_data in reviews:
        content = review_data.get("content", "").strip()
        if not content:
            continue

        try:
            upsert_review_to_db(db, place_id, review_data)
            success += 1
        except Exception as exc:
            failed += 1
            if failed <= 3:
                print(f"[FAIL] place_id={place_id}, review_id={review_data.get('id')} ì €ì¥ ì‹¤íŒ¨: {exc}", file=sys.stderr)

    return success, failed


async def crawl_reviews_from_db(
    place_ids: list[int] | None = None,
    max_count: int = 100,
    limit: int | None = None,
    headless: bool = True,
) -> None:
    """DBì— ì €ì¥ëœ ì¥ì†Œë“¤ì— ëŒ€í•´ ë¦¬ë·° í¬ë¡¤ë§ ë° ì €ì¥."""
    db = SessionLocal()
    try:
        # ì¥ì†Œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        if place_ids:
            target_ids = list(dict.fromkeys(place_ids))
        else:
            places = db.query(Place.id).all()
            target_ids = [p.id for p in places]

        if limit:
            target_ids = target_ids[:limit]

        print(f"ğŸ“‹ ì´ {len(target_ids)}ê°œ ì¥ì†Œì— ëŒ€í•´ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = NaverMapReviewCrawler(headless=headless, verbose=True)

        total_success = 0
        total_failed = 0
        places_processed = 0

        for i, place_id in enumerate(target_ids, 1):
            print(f"\n[{i}/{len(target_ids)}] place_id={place_id} ì²˜ë¦¬ ì¤‘...", file=sys.stderr)
            
            success, failed = await crawl_reviews_for_place(crawler, db, place_id, max_count)
            
            if success > 0:
                places_processed += 1
                total_success += success
                total_failed += failed
                print(f"[INFO] place_id={place_id}: {success}ê°œ ë¦¬ë·° ì €ì¥ ì™„ë£Œ", file=sys.stderr)
            else:
                print(f"[INFO] place_id={place_id}: ë¦¬ë·° ì—†ìŒ ë˜ëŠ” í¬ë¡¤ë§ ì‹¤íŒ¨", file=sys.stderr)

        print("\n" + "=" * 60)
        print("ë¦¬ë·° í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ")
        print("=" * 60)
        print(f"  ì²˜ë¦¬ëœ ì¥ì†Œ: {places_processed}ê°œ")
        print(f"  ì €ì¥ëœ ë¦¬ë·°: {total_success}ê°œ")
        print(f"  ì‹¤íŒ¨í•œ ë¦¬ë·°: {total_failed}ê°œ")
        print("=" * 60)

    finally:
        db.close()


def main() -> None:
    parser = argparse.ArgumentParser(description="DBì— ì €ì¥ëœ ì¥ì†Œë“¤ì— ëŒ€í•´ ë¦¬ë·° í¬ë¡¤ë§ ë° ì €ì¥")
    parser.add_argument(
        "--place-ids",
        type=str,
        help="ì²˜ë¦¬í•  place_id ëª©ë¡ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 123,456,789)",
    )
    parser.add_argument(
        "--max-count",
        type=int,
        default=100,
        help="ì¥ì†Œë³„ ìµœëŒ€ ë¦¬ë·° ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸: 100)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="ì²˜ë¦¬í•  ì¥ì†Œ ìµœëŒ€ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)",
    )
    parser.add_argument(
        "--headless",
        action="store_true",
        default=True,
        help="ë¸Œë¼ìš°ì €ë¥¼ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰ (ê¸°ë³¸: True)",
    )
    args = parser.parse_args()

    place_ids = None
    if args.place_ids:
        try:
            place_ids = [int(x.strip()) for x in args.place_ids.split(",") if x.strip()]
        except ValueError:
            raise SystemExit("--place-idsëŠ” ìˆ«ìë¡œë§Œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    asyncio.run(
        crawl_reviews_from_db(
            place_ids=place_ids,
            max_count=args.max_count,
            limit=args.limit,
            headless=args.headless,
        )
    )


if __name__ == "__main__":
    main()
